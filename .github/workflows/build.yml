name: build

on:
  workflow_dispatch:
  push:
    branches: [main]
  pull_request:
    branches: [main]

concurrency:
  group: "${{ github.workflow }}-${{ github.ref }}"
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    env:
      MNIST_FNAME: /tmp/mnist/mnist.ts.pt
      MNIST_COMMIT: 94b288a631362aa44edc219eb8f54a7c39891169
    steps:
      - uses: actions/checkout@v3
      - uses: cachix/install-nix-action@v18
        with:
          install_url: https://releases.nixos.org/nix/nix-2.13.3/install
          extra_nix_config: |
            substituters = https://cache.nixos.org https://cache.iog.io
            trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= hydra.iohk.io:f/Ea+s+dFdN+3Y/G+FDgSq+a5NEWhJGzdjvKNGv0/EQ=
      - uses: cachix/cachix-action@v8
        with:
          name: inferno
          authToken: "${{ secrets.CACHIX_TOKEN }}"
      - run: |
          nix build -L .#

      # Download and train MNIST model to test Inferno's torchscript model loader

      - name: Cache MNIST model
        id: cache-mnist
        uses: actions/cache@v3
        with:
          path: ${{ env.MNIST_FNAME }}
          key: ${{ env.MNIST_COMMIT }}
      - name: Download and train MNIST (if not cached)
        if: steps.cache-mnist.outputs.cache-hit != 'true'
        run: |
          cd "$(dirname ${MNIST_FNAME})"
          python -m pip install --upgrade pip
          # Need specific torch version for the serialized torchscript models (e.g. Bert) to be compatible with hasktorch:
          pip install torch==1.11.0+cpu torchvision==0.12.0+cpu torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cpu
          wget https://github.com/hasktorch/hasktorch/raw/${MNIST_COMMIT}/examples/model-serving/04-python-torchscript/mnist.py
          python mnist.py
      - name: Run inferno on MNIST
        working-directory: ${GITHUB_WORKSPACE}/inferno-ml/test/
        run: |
          cp ${MNIST_FNAME} ./
          nix run .#inferno-ml-exe -- mnist.inferno

      # Downhole autoencoder example
      - name: Download and run downhole autoencode model
        working-directory: ${GITHUB_WORKSPACE}/inferno-ml/test/
        run: |
          wget https://www.dropbox.com/s/gshxebydlwqvspj/downhole_autoencoder.ts.pt?dl=1
          nix run .#inferno-ml-exe -- downhole-autoencoder.inferno
